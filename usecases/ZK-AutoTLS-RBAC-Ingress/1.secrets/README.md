# Secrets

[![en](https://img.shields.io/badge/lang-en-red.svg)](https://github.com/ogomezso/cfk-runbooks/blob/main/usecases/ZK-AutoTLS-RBAC-Ingress/1.secrets/README.md)
[![es](https://img.shields.io/badge/lang-es-yellow.svg)](https://github.com/ogomezso/cfk-runbooks/blob/main/usecases/ZK-AutoTLS-RBAC-Ingress/1.secrets/README.es.md)

On this section we gonna go through the different K8s secrets we will need to put in place for our use case deployment

## Way of working

- Use local files provided on `1.secrets` as base to generate the K8s secrets
- Base on that we are going to dynamically create the yaml resource file for the secret using `kubectl` commands:

```bash
kubectl create secret <specific secret config> --dry-run:client -o yaml > generated/<client-secret-name>.yaml
```

these files are going to be generated under the `generated` folder

- On file based secret creation is important to take care of the destination file name since operator can only search for specific ones. This name is the one your provide on the left side of `from-file=` field of the kubectl command

## TLS secrets

We will need two secrets for internal and external tls configuration:

### Internal

As we already saw on the `certificates` section autogenerated secret is managed by `Confluent Operator` itself.

So we will need to create a `tls` kind of secret with the exact name of `ca-pair-sslcerts`:

Generate the resource file:

```bash
kubectl create secret tls ca-pair-sslcerts \
--cert=../0.certs/generated/CAcert.pem \
--key=--/0.certs/generated/CAkey.pem \
-n confluent --dry-run=client -output yaml > generated/ca-pair-sslcerts.yaml
```

and finally apply it:

```bash
kubectl apply -f generated/ca-pair-sslcerts.yaml
```

### External

For external access we would need two kinds of secret depending on the Ingress Controller ssl passthrough capabilities:

One which will be use for the operator to create the appropriate certificate stores on component side:

```bash
kubectl create secret generic kafka-external-tls \
  --from-file=fullchain.pem=../0.certs/generated/externalKafka.pem \
  --from-file=privkey.pem=../0.certs/generated/externalKafka-key.pem \
  --from-file=cacerts.pem=../0.certs/generated/CAcert.pem \
  --namespace confluent --dry-run=client -o yaml > generated/kafka-external-tls.yaml
```

and another for Ingress configuration in case of need:

```bash
kubectl create secret tls services-external-tls \
  --cert=../0.certs/generated/externalServices.pem \
  --key=../0.certs/generated/externalServices-key.pem \
  --namespace confluent --dry-run=client -o yaml > generated/services-external-tls.yaml
```

## Zookeeper

On this use case example `Zookeeper` service is implemented with `autogenerated certs` for connections (note that Kafka Brokers are the only client that zookeeper should have) and `digest` for authentication type.

For tls secret we just need to enable the `tls autogenerated` configuration and operator will use the already created `ca-pair-sslcerts`

For digest user we need to use a json file (provided as `zk-digest-cred.json` on this folder) as source for user information:

```json
{
    "zk": "zk-secret"
}
```

and generate the resource file from it:

```bash
kubectl create secret generic zk-digest \
  --from-file=digest-users.json=zk-digest-cred.json \
  --namespace confluent --dry-run=client -o yaml > generated/zk-digest.yaml
```

## Broker

On this example `Kafka Brokers` will use a variety of secrets depending on the use:

### MDS

MDS acts as a kind of oauth authorization service for Brokers, it's use to be deployed inside each broker on a specific port.

It will be in charge to communicate with your identity backend (LDAP on this example), store the user/groups available for our platform and its permission

Brokers as well as other Internal components will communicate with them in order to be authorized for a specific action.

We will need several secrets to put it to work:

First thing we need its a `key pair` for MDS to be able to sign tokens:

```bash
openssl genrsa -out generated/mds-key-priv.pem 2048
```

```bash
openssl rsa -in generated/mds-key-priv.pem -out PEM -pubout -out generated/mds-key-pub.pem
```

and then create a secret `mds-key-pair` to store it:

```bash
kubectl create secret generic mds-key-pair \
--from-file=mdsPublicKey.pem=generated/mds-key-pub.pem \
--from-file=mdsTokenKeyPair.pem=generated/mds-key-priv.pem \
--namespace confluent \
--dry-run=client \
--output yaml > generated/mds-key-pair.yaml
```

As we are using LDAP as users backend we will need a LDAP credentials for MDS to authenticate on it for this example we gonna use the provided `mds-ldap-creds.txt` file that content the LDAP user a password and create the `mds-ldap-creds` secret.

```bash
kubectl create secret generic mds-ldap-creds \
 --from-file=ldap.txt=mds-ldap-creds.txt \
 --namespace confluent \
 --dry-run=client \
 --output yaml > generated/mds-ldap-creds.yaml
```

### Internal Communication

For **internal tls**  secret we just need to enable the `tls autogenerated` configuration and operator will use the already created `ca-pair-sslcerts`

For **Replication Listener** (interbroker communication) we will need a  `SASL JaaS config passthough` file based secret:

```conf
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
  username="admin" password="admin-secret" \
  user_kafka="kafka-secret" user_admin="admin-secret";
```

On these configuration we are setting up the `admin` user as server user for `interbroker` communication and advertising that there are two possible users that can connect to Kafka: admin itself with password `admin-secret` and `kafka` user with password `kafka-secret` that will be the one that other components will use for broker to authenticate on the internal channel.

We rely on the provided `replication-sasl-users.conf` to create the `replication-sasl-cred` secret configured on the broker:

```bash
kubectl create secret generic replication-sasl-cred \
  --from-file=plain-jaas.conf=replication-sasl-users.conf \
  --namespace confluent --dry-run=client -o yaml > generated/replication-sasl-cred.yaml
```

On the **Internal Listener** (K8s internal clients) we gonna use LDAP as backend for users to authenticate authorize. That means that the user we configure on the client side must exist on LDAP and more specifically on the MDS cache.

But we still need to configure a user that the broker gonna use to authenticate on the configuration. In this case we will use a different simple `JaaS config` file based on a plain text file `kafka-internal-creds.txt`:

```txt
username=kafka
password=kafka-secret
```

We gonna create a secret call `kafka-internal-creds` to hold this secret

```bash
kubectl create secret generic kafka-internal-creds \
 --from-file=plain-interbroker.txt=kafka-internal-creds.txt \
 --namespace confluent \
 --dry-run=client \
 --output yaml > generated/kafka-internal-creds.yaml
```

As you can see here the user that we are using for brokers to authenticate is the one we configured as additional one on the `REPLICATION Listener`, `kafka`

### External Communication

As we gonna use **provided certificates** for external access we need to create the `kafka-external-tls` secret containing the `externalKafka` certificate create on the `certificates` section:

```bash
kubectl create secret generic kafka-external-tls \
  --from-file=fullchain.pem=../0.certs/generated/externalKafka.pem \
  --from-file=privkey.pem=../0.certs/generated/externalKafka-key.pem \
  --from-file=cacerts.pem=../0.certs/generated/ExternalCAcert.pem \
  --namespace confluent \
  --dry-run=client \
  -o yaml > generated/kafka-external-tls.yaml
```

Since we are using LDAP for external client authentication we don't need to provide any sort of whitelist users.

### Metric Reporter

For the Confluent Metric Reporter we gonna need a `outhbearer` secret created from the provided `kafka-bearer.txt` file that will serve to generate a MDS signed token. For this one we are going to use our **superUser** `kafka`

```bash
kubectl create secret generic kafka-bearer-creds \
 --from-file=bearer.txt=kafka-bearer.txt \
 --namespace confluent \
 --dry-run=client \
 -o yaml > generated/kafka-bearer-creds.yaml
```

### Services

On this use case we have `MDS` that will use the secrets already described on the MDS section.

### Dependencies

As dependency each broker has:

#### Zookeeper

For broker to connect to Zookeeper we need to provide a **digest** user and password but this time we gonna create the secret from a txt file: `zk-digest-cred.txt`:

```bash
kubectl create secret generic kafka-zk-digest-cred \
 --from-file=digest.txt=zk-digest-cred.txt \
 --namespace confluent \
 --dry-run=client \
 --output yaml > generated/kafka-zk-digest-cred.yaml
```

#### Kafka Rest Class

That provides a rest interface to Kafka APIs used for admin purpose (by operator between others)

In this case we going to use the same `kafka-bearer-creds` secret we used for `metrics reporter` before.
